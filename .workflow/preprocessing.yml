name: Retrain StudentsPerformance Model

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  retrain:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Miniconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          environment-file: Workflow-CI/MLProject/conda.yaml
          activate-environment: mlflow-env
          auto-activate-base: false

      - name: Verify Conda Environment
        shell: bash -l {0}
        run: |
          conda info
          conda list
          echo "Conda environment ready"

      - name: Ensure dataset exists
        shell: bash -l {0}
        run: |
          echo "Checking dataset file..."
          if [ ! -f "namadataset_raw/StudentsPerformance.csv" ]; then
            echo "Dataset not found! Please place StudentsPerformance.csv in namadataset_raw/"
            exit 1
          fi
          echo "Dataset found"

      - name: Run preprocessing
        shell: bash -l {0}
        run: |
          echo "Running preprocessing..."
          python preprocessing/automate_rahmi.py \
            --input namadataset_raw/StudentsPerformance.csv \
            --output preprocessing/namadataset_preprocessing/dataset_preprocessed.csv
          echo "Preprocessing completed"

      - name: Install MLflow (if not in env)
        shell: bash -l {0}
        run: |
          pip install --upgrade mlflow

      - name: Run MLflow Project (Training)
        shell: bash -l {0}
        run: |
          cd Workflow-CI/MLProject
          echo "Running MLflow project..."
          mlflow run . \
            -P input="../preprocessing/StudentsPerformance_preprocessing/dataset_preprocessed.csv" \
            -P target="math score"
          echo "Model trained successfully"

      - name: Show MLflow Run Summary
        shell: bash -l {0}
        run: |
          echo "Listing MLflow runs..."
          ls -R Workflow-CI/MLProject/mlruns || echo "No mlruns directory found!"
